import { tools } from '../tools/definitions.js';
import { AssessmentRagService } from './assessmentRagService.js';
import { getDb } from '../config/mongodb.js';
/**
 * Service responsible for generating AI-powered responses using OpenAI
 */
export default class CompletionService {
    openAIService;
    assessmentRagService;
    constructor(openAIService) {
        this.openAIService = openAIService;
        this.assessmentRagService = new AssessmentRagService(getDb());
    }
    /**
     * Generates a contextual response based on the user's query, intent, and relevant context
     * @param query - The user's input message
     * @param intent - The analyzed intent of the user's message
     * @param vectorResults - Relevant context vectors from the knowledge base
     * @param historySummary - Summary of the conversation history
     * @param messages - Previous messages in the conversation
     * @returns A generated contextual response
     */
    async generateContextualResponse({ query, context, vectorResults, historySummary, messages = [], }) {
        const vectorContext = this.formatVectorResults(vectorResults);
        const systemMessage = {
            role: 'system',
            content: process.env.SYSTEM_PROMPT,
        };
        let userMessages = [];
        if (query) {
            const content = [
                `User Query: "${query}"`,
                `User Profile Context: ${JSON.stringify(context, null, 2)}`,
                '',
                'Conversation Summary:',
                historySummary,
                '',
                'Relevant information from knowledge base:',
                vectorContext,
            ].join('\n');
            userMessages.push({ role: 'user', content });
        }
        // Convert any LangChain messages to OpenAI format
        const convertedMessages = this.convertMessagesToOpenAIFormat(messages);
        // Simple filter to remove any tool messages
        const safeMessages = convertedMessages.filter(message => message.role !== 'tool');
        // Debug logging
        const toolMessagesInConverted = convertedMessages.filter(msg => msg.role === 'tool');
        if (toolMessagesInConverted.length > 0) {
            console.log('âš ï¸ Found tool messages in converted messages:', toolMessagesInConverted);
        }
        const allMessages = [
            systemMessage,
            ...safeMessages,
            ...userMessages,
        ];
        console.log('Messages sent to OpenAI:', JSON.stringify(allMessages, null, 2));
        const response = await this.openAIService.createChatCompletion({
            messages: allMessages,
            tools,
        });
        const choice = response.choices[0];
        if (choice.finish_reason === 'tool_calls' && choice.message.tool_calls?.length) {
            const toolCall = choice.message.tool_calls[0];
            const { name, arguments: args } = toolCall.function;
            const parsedArgs = JSON.parse(args);
            if (name === 'suggest_assessment') {
                const suggestion = await this.handleAssessmentSuggestion(parsedArgs);
                return { role: 'assistant', content: suggestion, tool_calls: choice.message.tool_calls, refusal: "false" };
            }
            if (name === 'start_assessment') {
                const result = await this.handleStartAssessment(parsedArgs);
                return { role: 'assistant', content: result, tool_calls: choice.message.tool_calls, refusal: "false" };
            }
            if (name === 'process_assessment_answer') {
                const result = await this.handleProcessAssessmentAnswer(parsedArgs);
                return { role: 'assistant', content: result, tool_calls: choice.message.tool_calls, refusal: "false" };
            }
            return { role: 'assistant', content: '[FunÃ§Ã£o reconhecida, mas sem aÃ§Ã£o definida]', tool_calls: choice.message.tool_calls, refusal: "false" };
        }
        return choice.message;
    }
    /**
     * Converts LangChain messages to OpenAI format
     * @param messages - Array of messages that might be LangChain or OpenAI format
     * @returns Array of messages in OpenAI format
     */
    convertMessagesToOpenAIFormat(messages) {
        return messages.map(message => {
            // If it's already in OpenAI format, check if it's a tool message first
            if (message.role) {
                // Skip tool messages immediately
                if (message.role === 'tool') {
                    return null;
                }
                // Return other messages as-is
                if (message.content || message.tool_calls) {
                    return message;
                }
            }
            // If it's a LangChain message, convert it
            if (message._getType) {
                const type = message._getType();
                if (type === 'human') {
                    return { role: 'user', content: message.content };
                }
                else if (type === 'ai') {
                    return { role: 'assistant', content: message.content };
                }
                else if (type === 'system') {
                    return { role: 'system', content: message.content };
                }
                else if (type === 'tool') {
                    // Skip tool messages
                    return null;
                }
            }
            // If it's an unknown format, try to extract role and content
            if (message.role) {
                return message;
            }
            // Default fallback - treat as user message
            console.warn('Unknown message format, treating as user message:', message);
            return { role: 'user', content: message.content || JSON.stringify(message) };
        }).filter(message => message && message.role && (message.content || message.tool_calls));
    }
    /**
     * Handle assessment suggestion using RAG for intelligent suggestions
     */
    async handleAssessmentSuggestion(args) {
        const { user_id, user_query } = args;
        // Use RAG to get intelligent assessment suggestions
        const suggestions = await this.assessmentRagService.embeddingService.getAssessmentSuggestions(user_query);
        if (suggestions.length === 0) {
            return 'Desculpe, nÃ£o consegui identificar uma anÃ¡lise adequada para sua situaÃ§Ã£o. Pode me contar mais sobre o que vocÃª gostaria de melhorar no seu negÃ³cio?';
        }
        const bestSuggestion = suggestions[0];
        const assessmentDefinitions = this.assessmentRagService.getAvailableAssessments();
        const assessment = assessmentDefinitions.find(a => a.name === bestSuggestion.suggestedAssessment);
        if (!assessment) {
            return 'Desculpe, nÃ£o consegui identificar uma anÃ¡lise adequada para sua situaÃ§Ã£o.';
        }
        let message = `ðŸ’¡ Baseado na sua pergunta sobre "${user_query}", sugiro a anÃ¡lise: **${assessment.name}**\n\n`;
        message += `ðŸ“‹ **O que esta anÃ¡lise faz:**\n${assessment.description}\n\n`;
        message += `ðŸ¤” **Por que seria Ãºtil:** ${bestSuggestion.reasoning}\n\n`;
        message += `âœ… **Gostaria de comeÃ§ar esta anÃ¡lise agora?**\n`;
        message += `Responda "sim" para iniciar ou me diga se prefere outra abordagem.`;
        return message;
    }
    /**
     * Handle starting an assessment using the AssessmentService directly
     */
    async handleStartAssessment(args) {
        const { assessment_name, user_id } = args;
        try {
            const result = await this.assessmentRagService.assessmentService.startAssessment(assessment_name, user_id);
            if (result.currentStep) {
                return result.currentStep.goal_prompt;
            }
            return `Vamos comeÃ§ar a anÃ¡lise de ${assessment_name}. Por favor, forneÃ§a as informaÃ§Ãµes necessÃ¡rias.`;
        }
        catch (error) {
            console.error('Error starting assessment:', error);
            return 'Desculpe, houve um erro ao iniciar a anÃ¡lise. Tente novamente.';
        }
    }
    /**
     * Handle processing assessment answers using the AssessmentService directly
     */
    async handleProcessAssessmentAnswer(args) {
        const { user_id, input } = args;
        try {
            // Get the current assessment from user profile
            const user = await getDb().collection("user_profiles").findOne({ _id: user_id });
            const currentAssessment = user?.progress?.currentAssessment;
            if (!currentAssessment) {
                return 'NÃ£o hÃ¡ uma anÃ¡lise ativa no momento.';
            }
            const result = await this.assessmentRagService.assessmentService.processAnswer(currentAssessment, user_id, input);
            if (result.status === 'completed') {
                let message = `âœ… AnÃ¡lise de ${currentAssessment} concluÃ­da!\n\n`;
                if (result.insights && result.insights.length > 0) {
                    message += "ðŸ’¡ Principais insights:\n";
                    result.insights.forEach((insight, index) => {
                        message += `${index + 1}. ${insight}\n`;
                    });
                }
                return message;
            }
            if (result.nextStep) {
                return result.nextStep.goal_prompt;
            }
            return 'Por favor, continue respondendo as perguntas da anÃ¡lise.';
        }
        catch (error) {
            console.error('Error processing assessment answer:', error);
            return 'Desculpe, houve um erro ao processar sua resposta. Tente novamente.';
        }
    }
    /**
     * Formats vector results into a readable string
     * @param results - Array of vector results to format
     * @returns Formatted string of vector results
     * @private
     */
    formatVectorResults(results) {
        return results.map((result, index) => `${index + 1}. ${result.text}`).join('\n');
    }
}
//# sourceMappingURL=completion.service.js.map